å‰ç«¯å½•éŸ³é€šè¿‡wså‘é€äºŒè¿›åˆ¶éŸ³é¢‘(pcm)åˆ°åç«¯fastapi,åç«¯ä½¿ç”¨sherpa_onnxå¤„ç†vadå’Œkwsã€‚è‡ªå®šä¹‰å”¤é†’è¯ä¸º["ä½ å¥½å°ç«‹","å°ç«‹å°ç«‹", "å°ç«‹åŒå­¦"]ã€‚æµå¼å¤„ç†vadå’Œkws

- 1. æ¨¡å‹æˆ‘å·²ç»ä¸‹è½½
- 2. å‚è€ƒsherpa_onnxçš„ç¤ºä¾‹ä»£ç å¦‚ä¸‹ï¼š
#!/usr/bin/env python3
"""
Sherpa-ONNX ä¸­æ–‡å”¤é†’è¯å®æ—¶æ£€æµ‹ç¤ºä¾‹
æ”¯æŒä»æ–‡ä»¶å’Œéº¦å…‹é£å®æ—¶æ£€æµ‹
"""

import sys
import wave
import numpy as np
import sherpa_onnx

def read_wave(wave_filename: str):
    """
    è¯»å– WAV æ–‡ä»¶
    è¿”å›: (éŸ³é¢‘æ•°æ®, é‡‡æ ·ç‡)
    """
    with wave.open(wave_filename) as f:
        assert f.getnchannels() == 1, "ä»…æ”¯æŒå•å£°é“éŸ³é¢‘"
        assert f.getsampwidth() == 2, "ä»…æ”¯æŒ 16-bit éŸ³é¢‘"
        
        samples = f.readframes(f.getnframes())
        samples_int16 = np.frombuffer(samples, dtype=np.int16)
        samples_float32 = samples_int16.astype(np.float32) / 32768.0
        
        return samples_float32, f.getframerate()


def create_keyword_spotter(model_dir: str, keywords_file: str):
    """
    åˆ›å»ºå”¤é†’è¯æ£€æµ‹å™¨
    
    å‚æ•°:
        model_dir: æ¨¡å‹ç›®å½•è·¯å¾„
        keywords_file: å…³é”®è¯æ–‡ä»¶è·¯å¾„
    """
    print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_dir}")
    
    kws = sherpa_onnx.KeywordSpotter(
        tokens=f"{model_dir}/tokens.txt",
        encoder=f"{model_dir}/encoder-epoch-12-avg-2-chunk-16-left-64.onnx",
        decoder=f"{model_dir}/decoder-epoch-12-avg-2-chunk-16-left-64.onnx",
        joiner=f"{model_dir}/joiner-epoch-12-avg-2-chunk-16-left-64.onnx",
        num_threads=2,
        keywords_file=keywords_file,
        provider="cpu",  # å¯æ”¹ä¸º "cuda" å¦‚æœæœ‰ GPU
        max_active_paths=4,
        num_trailing_blanks=1,
        keywords_score=1.0,
        keywords_threshold=0.25,
    )
    
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
    return kws


def detect_from_file(kws, audio_file: str):
    """ä»éŸ³é¢‘æ–‡ä»¶æ£€æµ‹å”¤é†’è¯"""
    print(f"\nğŸ“ å¤„ç†æ–‡ä»¶: {audio_file}")
    
    samples, sample_rate = read_wave(audio_file)
    
    # åˆ›å»ºéŸ³é¢‘æµ
    stream = kws.create_stream()
    stream.accept_waveform(sample_rate, samples)
    
    # è¾“å…¥ç»“æŸä¿¡å·
    tail_paddings = np.zeros(int(0.3 * sample_rate), dtype=np.float32)
    stream.accept_waveform(sample_rate, tail_paddings)
    stream.input_finished()
    
    # æ£€æµ‹
    while kws.is_ready(stream):
        kws.decode_stream(stream)
    
    keyword = kws.get_result(stream).keyword
    
    if keyword:
        print(f"ğŸ¯ æ£€æµ‹åˆ°å”¤é†’è¯: {keyword}")
    else:
        print("âŒ æœªæ£€æµ‹åˆ°å”¤é†’è¯")
    
    return keyword


def detect_from_microphone(kws):
    """ä»éº¦å…‹é£å®æ—¶æ£€æµ‹å”¤é†’è¯"""
    try:
        import pyaudio
    except ImportError:
        print("âŒ è¯·å…ˆå®‰è£… pyaudio: pip install pyaudio")
        return
    
    print("\nğŸ¤ å¼€å§‹ç›‘å¬éº¦å…‹é£...")
    print("æŒ‰ Ctrl+C åœæ­¢")
    
    sample_rate = 16000
    samples_per_read = int(0.1 * sample_rate)  # 100ms
    
    p = pyaudio.PyAudio()
    stream_audio = p.open(
        format=pyaudio.paInt16,
        channels=1,
        rate=sample_rate,
        input=True,
        frames_per_buffer=samples_per_read,
    )
    
    stream = kws.create_stream()
    
    try:
        while True:
            # è¯»å–éŸ³é¢‘æ•°æ®
            data = stream_audio.read(samples_per_read, exception_on_overflow=False)
            samples = np.frombuffer(data, dtype=np.int16)
            samples = samples.astype(np.float32) / 32768.0
            
            # é€å…¥æ£€æµ‹å™¨
            stream.accept_waveform(sample_rate, samples)
            
            # æ£€æµ‹
            while kws.is_ready(stream):
                kws.decode_stream(stream)
                result = kws.get_result(stream)
                
                if result.keyword:
                    print(f"\nğŸ”¥ æ£€æµ‹åˆ°å”¤é†’è¯: '{result.keyword}' ğŸ”¥")
                    print(f"   æ—¶é—´æˆ³: {result.start_time:.2f}s")
                    # è¿™é‡Œå¯ä»¥æ·»åŠ ä½ çš„å”¤é†’åé€»è¾‘
                    
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸  åœæ­¢ç›‘å¬")
    finally:
        stream_audio.stop_stream()
        stream_audio.close()
        p.terminate()


def create_keywords_file(keywords: list, filename: str = "keywords.txt"):
    """
    åˆ›å»ºå…³é”®è¯æ–‡ä»¶
    
    å‚æ•°:
        keywords: å…³é”®è¯åˆ—è¡¨ï¼Œä¾‹å¦‚ ["å°çˆ±åŒå­¦", "ä½ å¥½å°æ™º", "æ‰“å¼€åŠ©æ‰‹"]
        filename: è¾“å‡ºæ–‡ä»¶å
    """
    with open(filename, "w", encoding="utf-8") as f:
        for kw in keywords:
            f.write(f"{kw}\n")
    print(f"âœ… å…³é”®è¯æ–‡ä»¶å·²åˆ›å»º: {filename}")
    return filename


def main():
    # é…ç½®
    MODEL_DIR = "./sherpa-onnx-kws-zipformer-wenetspeech-3.3M-2024-01-01"
    
    # å®šä¹‰ä½ çš„å”¤é†’è¯ï¼ˆå¯ä»¥æ˜¯å¤šä¸ªï¼‰
    MY_KEYWORDS = [
        "å°çˆ±åŒå­¦",
        "ä½ å¥½å°æ™º", 
        "æ‰“å¼€åŠ©æ‰‹",
        "å˜¿å°æ˜"
    ]
    
    # åˆ›å»ºå…³é”®è¯æ–‡ä»¶
    keywords_file = create_keywords_file(MY_KEYWORDS, "my_keywords.txt")
    
    # åˆ›å»ºæ£€æµ‹å™¨
    kws = create_keyword_spotter(MODEL_DIR, keywords_file)
    
    # é€‰æ‹©æ¨¡å¼
    print("\n" + "="*50)
    print("é€‰æ‹©æ£€æµ‹æ¨¡å¼ï¼š")
    print("1. ä»éº¦å…‹é£å®æ—¶æ£€æµ‹")
    print("2. ä»éŸ³é¢‘æ–‡ä»¶æ£€æµ‹")
    print("="*50)
    
    choice = input("è¯·è¾“å…¥é€‰é¡¹ (1/2): ").strip()
    
    if choice == "1":
        detect_from_microphone(kws)
    elif choice == "2":
        # æµ‹è¯•æ–‡ä»¶æ£€æµ‹
        test_files = [
            f"{MODEL_DIR}/test_wavs/1.wav",
            f"{MODEL_DIR}/test_wavs/2.wav",
            f"{MODEL_DIR}/test_wavs/3.wav",
        ]
        
        for audio_file in test_files:
            try:
                detect_from_file(kws, audio_file)
            except FileNotFoundError:
                print(f"âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {audio_file}")
    else:
        print("âŒ æ— æ•ˆé€‰é¡¹")


if __name__ == "__main__":
    main()

- 3. å‚è€ƒèµ„æ–™ï¼š
https://github.com/k2-fsa/sherpa-onnx
https://github.com/k2-fsa/sherpa-onnx/blob/master/python-api-examples

- 4. ä½¿ç”¨uvç®¡ç†pythonç‰ˆæœ¬å’Œä¾èµ– pythonç‰ˆæœ¬ä½¿ç”¨3.12.9
