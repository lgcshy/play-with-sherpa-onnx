"""
å…³é”®è¯æ£€æµ‹å™¨å°è£…ç±»
"""
import numpy as np
import sherpa_onnx
from pathlib import Path
from typing import Optional, List, Dict, Any
from loguru import logger

# pypinyin ç”¨äºå°†ä¸­æ–‡è½¬ä¸ºæ‹¼éŸ³ï¼ˆå«å£°æ¯/éŸµæ¯å’Œå£°è°ƒï¼‰
try:
    from pypinyin import pinyin, Style
except Exception:
    pinyin = None
    Style = None

from ..config import MODEL_DIR, CUSTOM_KEYWORDS
from .vad_detector import SileroVAD


class KeywordSpotter:
    """å…³é”®è¯æ£€æµ‹å™¨å°è£…ç±»"""
    
    def __init__(self, model_dir: str = None, keywords: List[str] = None):
        """
        åˆå§‹åŒ–å…³é”®è¯æ£€æµ‹å™¨
        
        Args:
            model_dir: æ¨¡å‹ç›®å½•è·¯å¾„
            keywords: è‡ªå®šä¹‰å…³é”®è¯åˆ—è¡¨
        """
        self.model_dir = Path(model_dir) if model_dir else MODEL_DIR
        self.keywords = keywords or CUSTOM_KEYWORDS
        self.kws = None
        self.keywords_file = None
        
        # åˆ›å»ºå…³é”®è¯æ–‡ä»¶
        self._create_keywords_file()
        
        # åˆå§‹åŒ–æ£€æµ‹å™¨
        self._initialize_spotter()
        
        # åˆå§‹åŒ–VADæ£€æµ‹å™¨
        self.vad = SileroVAD(self.model_dir)
    
    def _hanzi_to_token_line(self, text: str) -> str:
        """å°†ä¸­æ–‡è½¬æ¢ä¸ºéŸ³ç´ æ ¼å¼ï¼Œæ”¯æŒboosting scoreå’Œtrigger threshold"""
        if not pinyin or not Style:
            return text  # é€€åŒ–ï¼šä»ç„¶å†™åŸæ–‡
        
        # è§£æboosting scoreå’Œtrigger threshold
        boosting_score = ""
        trigger_threshold = ""
        original_text = text
        
        # æå–boosting score (æ ¼å¼: :1.5)
        if " :" in text:
            parts = text.split(" :")
            original_text = parts[0]
            if len(parts) > 1:
                remaining = parts[1]
                if " #" in remaining:
                    score_parts = remaining.split(" #")
                    boosting_score = f" :{score_parts[0]}"
                    if len(score_parts) > 1:
                        trigger_threshold = f" #{score_parts[1]}"
                else:
                    boosting_score = f" :{remaining}"
        
        # è·å–æ¯ä¸ªå­—çš„å£°æ¯ã€éŸµæ¯ï¼ˆå¸¦è°ƒï¼‰
        initials = pinyin(original_text, style=Style.INITIALS, strict=False, errors="ignore")
        finals = pinyin(original_text, style=Style.FINALS_TONE, strict=False, errors="ignore")

        tokens = []
        for (ini_list, fin_list) in zip(initials, finals):
            ini = (ini_list[0] or "").strip()
            fin = (fin_list[0] or "").strip()
            # å¯èƒ½é‡åˆ°éæ±‰å­—æˆ–è¢«å¿½ç•¥å†…å®¹
            if not ini and not fin:
                continue
            if ini:
                tokens.append(ini)
            if fin:
                tokens.append(fin)
        
        token_str = " ".join(tokens)
        # åœ¨æœ«å°¾è¿½åŠ ä¸­æ–‡å±•ç¤ºç”¨æ ‡ç­¾ï¼Œä¾¿äºç»“æœæ˜¾ç¤º
        result = f"{token_str}{boosting_score}{trigger_threshold} @{original_text}" if token_str else original_text
        return result

    def _create_keywords_file(self):
        """åˆ›å»ºå…³é”®è¯æ–‡ä»¶"""
        # åˆ›å»ºè‡ªå®šä¹‰å…³é”®è¯æ–‡ä»¶
        self.keywords_file = self.model_dir / "custom_keywords.txt"
        
        # å°†ä¸­æ–‡å…³é”®è¯è½¬æ¢ä¸ºéŸ³ç´ æ ¼å¼
        converted_lines = [self._hanzi_to_token_line(kw) for kw in self.keywords]
        
        with open(self.keywords_file, "w", encoding="utf-8") as f:
            for line in converted_lines:
                f.write(f"{line}\n")
        
        if not pinyin:
            logger.warning("âš ï¸ æœªå®‰è£… pypinyinï¼Œå·²æŒ‰åŸæ–‡å†™å…¥ã€‚å»ºè®®å®‰è£…ï¼špip install pypinyinã€‚å¦åˆ™ä¼šå‡ºç° tokens æ— æ³•åŒ¹é…çš„é”™è¯¯ã€‚")
        
        logger.info(f"âœ… å…³é”®è¯æ–‡ä»¶å·²åˆ›å»º: {self.keywords_file}")
        logger.info(f"å…³é”®è¯åˆ—è¡¨: {self.keywords}")
        logger.debug(f"è½¬æ¢åçš„éŸ³ç´ æ ¼å¼: {converted_lines}")
    
    def _initialize_spotter(self):
        """åˆå§‹åŒ–å…³é”®è¯æ£€æµ‹å™¨"""
        try:
            logger.info(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {self.model_dir}")
            
            self.kws = sherpa_onnx.KeywordSpotter(
                tokens=str(self.model_dir / "tokens.txt"),
                encoder=str(self.model_dir / "encoder-epoch-12-avg-2-chunk-16-left-64.onnx"),
                decoder=str(self.model_dir / "decoder-epoch-12-avg-2-chunk-16-left-64.onnx"),
                joiner=str(self.model_dir / "joiner-epoch-12-avg-2-chunk-16-left-64.onnx"),
                num_threads=2,
                keywords_file=str(self.keywords_file),
                provider="cpu",  # å¯æ”¹ä¸º "cuda" å¦‚æœæœ‰ GPU
                max_active_paths=4,
                num_trailing_blanks=1,
                keywords_score=1.0,
                keywords_threshold=0.0001,  # æä½é˜ˆå€¼ï¼Œå‡ ä¹ä»»ä½•éŸ³é¢‘éƒ½ä¼šè§¦å‘
            )
            
            logger.success("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def create_stream(self):
        """åˆ›å»ºéŸ³é¢‘æµ"""
        if not self.kws:
            raise RuntimeError("æ£€æµ‹å™¨æœªåˆå§‹åŒ–")
        return self.kws.create_stream()
    
    def process_audio_chunk(self, stream, audio_data: np.ndarray, sample_rate: int = 16000) -> Optional[str]:
        """
        å¤„ç†éŸ³é¢‘æ•°æ®å—ï¼ˆé›†æˆVADå’ŒKWSï¼‰
        
        Args:
            stream: éŸ³é¢‘æµå¯¹è±¡
            audio_data: éŸ³é¢‘æ•°æ® (numpy array)
            sample_rate: é‡‡æ ·ç‡
            
        Returns:
            æ£€æµ‹åˆ°çš„å…³é”®è¯ï¼Œå¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°åˆ™è¿”å›None
        """
        try:
            # å‡å°‘æ—¥å¿—é¢‘ç‡ - æ¯50ä¸ªéŸ³é¢‘å—è¾“å‡ºä¸€æ¬¡
            if hasattr(self, '_kws_count'):
                self._kws_count += 1
            else:
                self._kws_count = 1
                
            if self._kws_count % 50 == 0:
                logger.info(f"ğŸ¯ KWSå¤„ç†éŸ³é¢‘: {len(audio_data)} æ ·æœ¬")
            
            # ç›´æ¥è¿›è¡Œå…³é”®è¯æ£€æµ‹ï¼ˆVADåœ¨æµæ°´çº¿å±‚é¢å¤„ç†ï¼‰
            stream.accept_waveform(sample_rate, audio_data)
            
            # æ£€æµ‹å…³é”®è¯ - æ¯æ¬¡decode_streamåéƒ½æ£€æŸ¥ç»“æœ
            while self.kws.is_ready(stream):
                self.kws.decode_stream(stream)
                
                # æ¯æ¬¡è§£ç åéƒ½æ£€æŸ¥ç»“æœ
                result = self.kws.get_result(stream)
                keyword = result if isinstance(result, str) else getattr(result, "keyword", "")
                
                if keyword and keyword.strip():
                    logger.info(f"ğŸ¯ æ£€æµ‹åˆ°å”¤é†’è¯: '{keyword.strip()}'")
                    # æ£€æµ‹åˆ°å…³é”®è¯åé‡ç½®streamçŠ¶æ€ï¼Œä»¥ä¾¿ä¸‹æ¬¡æ£€æµ‹
                    self.kws.reset_stream(stream)
                    return keyword.strip()
            
            return None
            
        except Exception as e:
            logger.error(f"âŒ KWSéŸ³é¢‘å¤„ç†é”™è¯¯: {e}")
            import traceback
            logger.error(f"âŒ KWSé”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            return None
    
    def process_audio_file(self, audio_file: str) -> Optional[str]:
        """
        å¤„ç†éŸ³é¢‘æ–‡ä»¶
        
        Args:
            audio_file: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            
        Returns:
            æ£€æµ‹åˆ°çš„å…³é”®è¯
        """
        try:
            import wave
            
            with wave.open(audio_file) as f:
                assert f.getnchannels() == 1, "ä»…æ”¯æŒå•å£°é“éŸ³é¢‘"
                assert f.getsampwidth() == 2, "ä»…æ”¯æŒ 16-bit éŸ³é¢‘"
                
                samples = f.readframes(f.getnframes())
                samples_int16 = np.frombuffer(samples, dtype=np.int16)
                samples_float32 = samples_int16.astype(np.float32) / 32768.0
                
                sample_rate = f.getframerate()
            
            # åˆ›å»ºéŸ³é¢‘æµ
            stream = self.create_stream()
            stream.accept_waveform(sample_rate, samples_float32)
            
            # è¾“å…¥ç»“æŸä¿¡å·
            tail_paddings = np.zeros(int(0.3 * sample_rate), dtype=np.float32)
            stream.accept_waveform(sample_rate, tail_paddings)
            stream.input_finished()
            
            # æ£€æµ‹
            while self.kws.is_ready(stream):
                self.kws.decode_stream(stream)
            
            result = self.kws.get_result(stream)
            
            # å…¼å®¹ä¸åŒç‰ˆæœ¬çš„è¿”å›æ ¼å¼
            keyword = result if isinstance(result, str) else getattr(result, "keyword", "")
            
            if keyword and keyword.strip():
                logger.info(f"ğŸ¯ æ–‡ä»¶æ£€æµ‹åˆ°å”¤é†’è¯: '{keyword.strip()}'")
                return keyword.strip()
            else:
                logger.info("âŒ æ–‡ä»¶æœªæ£€æµ‹åˆ°å”¤é†’è¯")
                return None
                
        except Exception as e:
            logger.error(f"æ–‡ä»¶å¤„ç†é”™è¯¯: {e}")
            return None
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        kws_info = {
            "model_dir": str(self.model_dir),
            "keywords": self.keywords,
            "keywords_file": str(self.keywords_file),
            "sample_rate": 16000,
            "threshold": 0.1  # ä¸åˆå§‹åŒ–æ—¶çš„é˜ˆå€¼ä¿æŒä¸€è‡´
        }
        
        # æ·»åŠ VADä¿¡æ¯
        vad_info = self.vad.get_model_info()
        kws_info["vad"] = vad_info
        
        return kws_info
    
    def reset_vad(self):
        """é‡ç½®VADçŠ¶æ€"""
        if self.vad:
            self.vad.reset()
            logger.debug("å…³é”®è¯æ£€æµ‹å™¨VADçŠ¶æ€å·²é‡ç½®")
